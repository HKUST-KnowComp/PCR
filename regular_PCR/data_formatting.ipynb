{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import tempfile\n",
    "import subprocess\n",
    "import collections\n",
    "\n",
    "# import util\n",
    "import conll\n",
    "from bert import tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the medical data to the spanbert format\n",
    "raw_train = []\n",
    "raw_test = []\n",
    "\n",
    "with open(\"./data/medical_train.jsonlines\", encoding=\"utf-8\") as f:\n",
    "    for line in f.readlines():\n",
    "        raw_train.append(json.loads(line))\n",
    "        \n",
    "with open(\"./data/medical_test.jsonlines\", encoding=\"utf-8\") as f:\n",
    "    for line in f.readlines():\n",
    "        raw_test.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['sentences', 'clusters', 'doc_key', 'speakers'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = \"./cased_config_vocab/vocab.txt\"\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file=vocab_file, do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_lens =  [128, 256, 384, 512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_medic(raw, tokenizer, max_seg_len):\n",
    "    new = dict()\n",
    "    new[\"doc_key\"] = raw[\"doc_key\"]\n",
    "    \n",
    "    new[\"clusters\"] = [] \n",
    "    new[\"sentences\"] = [] #\n",
    "    new[\"speakers\"] = [] #\n",
    "    new[\"sentence_map\"] = [] #\n",
    "    new[\"subtoken_map\"] = [] #\n",
    "    \n",
    "    # new[\"segments\"] = []\n",
    "    \n",
    "    new_tokens = []\n",
    "    speaker_id = raw[\"speakers\"][0][0]\n",
    "    \n",
    "    sentence_tokens = flatten(raw[\"sentences\"])\n",
    "    \n",
    "#     cluster_map = []\n",
    "#     for token in sentence_tokens:\n",
    "#         cluster_map.append(0)\n",
    "        \n",
    "#     for i, cluster in enumerate(clusters):\n",
    "#         new[\"clusters\"].append([]) # cluster prototype\n",
    "#         for const in clusters:\n",
    "#             for j in range(const[0],(const[1]+1)):\n",
    "#                 cluster_map[j] = i+1\n",
    "                \n",
    "    new_cluster_map = []\n",
    "    word_idx = 0\n",
    "    seg_count = 0\n",
    "    subtoken_count = 0\n",
    "    \n",
    "    \n",
    "    for i,sentence in enumerate(raw[\"sentences\"]):\n",
    "\n",
    "        if seg_count == 0:\n",
    "            temp_segs=[\"[CLS]\"]\n",
    "            new[\"subtoken_map\"].append(word_idx)\n",
    "            new[\"sentence_map\"].append(i)# for cls\n",
    "            seg_count += 1\n",
    "            subtoken_count += 1\n",
    "        \n",
    "        # temp_speakers.append(\"[SPL]\")\n",
    "        \n",
    "        for j,token in enumerate(sentence):\n",
    "            # visit each token            \n",
    "            if token == \"/.\" or token == \"/?\" :\n",
    "                token = token[1:]\n",
    "        \n",
    "#             if word_idx in [82,83,84]:\n",
    "#                 print(word_idx)\n",
    "#                 print(token)\n",
    "            \n",
    "            subtokens = tokenizer.tokenize(token)\n",
    "            \n",
    "            if token == \"\":\n",
    "                subtokens = [\"[UNK]\"]\n",
    "            \n",
    "#             if word_idx in [82,83,84]:\n",
    "#                 print(subtokens)\n",
    "            \n",
    "            for sidx, subtoken in enumerate(subtokens):\n",
    "                # temp_tokens.append(subtokens)\n",
    "                temp_segs.append(subtoken)\n",
    "                new[\"subtoken_map\"].append(word_idx)\n",
    "                new[\"sentence_map\"].append(i)\n",
    "                seg_count += 1\n",
    "                subtoken_count += 1\n",
    "                \n",
    "                final_end_condition = (i==(len(raw[\"sentences\"])-1) and j==(len(sentence)-1) and sidx==(len(subtoken)-1))\n",
    "                \n",
    "                if seg_count == (max_seg_len-1) or final_end_condition:\n",
    "                    # temp_tokens.append(\"[SEP]\")\n",
    "                    # temp_speakers.append(\"[SPL]\")\n",
    "                    temp_segs.append(\"[SEP]\")\n",
    "                    new[\"sentences\"].append(temp_segs)\n",
    "                    new[\"subtoken_map\"].append(word_idx)\n",
    "                    new[\"sentence_map\"].append(i)\n",
    "                    \n",
    "                    tmp_speaker = [\"[SPL]\"]\n",
    "                    for tsg in range(len(temp_segs)-2):\n",
    "                        tmp_speaker.append(speaker_id)\n",
    "                    tmp_speaker.append(\"[SPL]\")\n",
    "                    new[\"speakers\"].append(tmp_speaker)\n",
    "                    \n",
    "                    subtoken_count += 1\n",
    "                    \n",
    "                    if not final_end_condition:\n",
    "                        temp_segs = [\"[CLS]\"]\n",
    "                        new[\"subtoken_map\"].append(word_idx)\n",
    "                        new[\"sentence_map\"].append(i)\n",
    "                        subtoken_count += 1\n",
    "                        seg_count = 1\n",
    "\n",
    "            word_idx += 1\n",
    "    \n",
    "    all_tokens = flatten(new[\"sentences\"])\n",
    "            \n",
    "    # dealing with the clusters:\n",
    "    for cluster in raw[\"clusters\"]:\n",
    "        temp_cluster = []\n",
    "        for consts in cluster:\n",
    "            new_start = new[\"subtoken_map\"].index(consts[0])\n",
    "            new_end = len(new[\"subtoken_map\"]) - new[\"subtoken_map\"][::-1].index(consts[1]) - 1\n",
    "            temp_cluster.append([new_start, new_end])\n",
    "            # print(all_tokens[new_start:new_end+1])\n",
    "        new[\"clusters\"].append(temp_cluster)\n",
    "        \n",
    "    \n",
    "    # checking\n",
    "    # merge clusters\n",
    "#     merged_clusters = []\n",
    "#     for c1 in new[\"clusters\"]:\n",
    "#         existing = None\n",
    "#         for m in c1:\n",
    "#             for c2 in merged_clusters:\n",
    "#                 if m in c2:\n",
    "#                     existing = c2\n",
    "#                     break\n",
    "#             if existing is not None:\n",
    "#                 break\n",
    "#         if existing is not None:\n",
    "#             print(\"Merging clusters (shouldn't happen very often.)\")\n",
    "#             existing.update(c1)\n",
    "#         else:\n",
    "#             merged_clusters.append(set(c1))\n",
    "#     merged_clusters = [list(c) for c in merged_clusters]\n",
    "    \n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = flatten(raw_train[55][\"sentences\"])\n",
    "# print(len(tokens))\n",
    "# print(tokens[80:86])\n",
    "# print(tokens[83])\n",
    "\n",
    "new_piece = process_medic(raw_train[55], tokenizer, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_medic(raw_train[0], tokenizer, max_seg_len=128)\n",
    "for seg_len in seg_lens:\n",
    "    with open(\"./data/train.medical.\"+str(seg_len)+\".jsonlines\",\"w\") as f:\n",
    "        for i,raw_piece in enumerate(raw_train):\n",
    "            # print(i)\n",
    "            new_piece = process_medic(raw_piece, tokenizer, seg_len)\n",
    "            f.write(json.dumps(new_piece))\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "    with open(\"./data/test.medical.\"+str(seg_len)+\".jsonlines\",\"w\") as f:\n",
    "        for raw_piece in raw_test:\n",
    "            new_piece = process_medic(raw_piece, tokenizer, seg_len)\n",
    "            f.write(json.dumps(new_piece))\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
