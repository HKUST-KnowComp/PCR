{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load WSC dataset\n",
    "\n",
    "import xml.etree.ElementTree as etree\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "import logging\n",
    "import numpy\n",
    "import os\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x)/sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': {'txt1': 'The city councilmen refused the demonstrators a permit because', 'pron': 'they', 'txt2': 'feared violence.'}, 'answers': ['The city councilmen', 'The demonstrators'], 'correctAnswer': 'A', 'source': '(Winograd1972)'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xzhaoar/anaconda3/envs/gpt2/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: This method will be removed in future versions.  Use 'list(elem)' or iteration over elem instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/xzhaoar/anaconda3/envs/gpt2/lib/python3.7/site-packages/ipykernel_launcher.py:8: DeprecationWarning: This method will be removed in future versions.  Use 'list(elem)' or iteration over elem instead.\n",
      "  \n",
      "/home/xzhaoar/anaconda3/envs/gpt2/lib/python3.7/site-packages/ipykernel_launcher.py:16: DeprecationWarning: This method will be removed in future versions.  Use 'list(elem)' or iteration over elem instead.\n",
      "  app.launch_new_instance()\n",
      "/home/xzhaoar/anaconda3/envs/gpt2/lib/python3.7/site-packages/ipykernel_launcher.py:10: DeprecationWarning: This method will be removed in future versions.  Use 'list(elem)' or iteration over elem instead.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "tree = etree.parse('WSCollection.xml')\n",
    "root = tree.getroot()\n",
    "original_problems = root.getchildren()\n",
    "problems = list()\n",
    "\n",
    "for original_problem in original_problems:\n",
    "    problem = dict()\n",
    "    for information in original_problem.getchildren():\n",
    "        if information.tag == 'answers':\n",
    "            answers = information.getchildren()\n",
    "            answer_list = list()\n",
    "            for answer in answers:\n",
    "                answer_list.append(answer.text.strip())\n",
    "            problem['answers'] = answer_list\n",
    "        elif information.tag == 'text':\n",
    "            texts = information.getchildren()\n",
    "            text_dict = dict()\n",
    "            for text1 in texts:\n",
    "                text_dict[text1.tag] = text1.text.replace('\\n', ' ').strip()\n",
    "            problem['text'] = text_dict\n",
    "        elif information.tag == 'quote':\n",
    "            pass\n",
    "        else:\n",
    "            problem[information.tag] = information.text.replace(' ', '')\n",
    "    problems.append(problem)\n",
    "\n",
    "print(problems[0])\n",
    "\n",
    "all_sentences = list()\n",
    "for question in problems:\n",
    "    sentence = question['text']['txt1'] + ' ' + question['text']['pron'] + ' ' + question['text']['txt2']\n",
    "    all_sentences.append(sentence)\n",
    "    # print(sentence)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0730 18:08:22.649106 140254510032704 deprecation_wrapper.py:119] From /home/xzhaoar/gpt-2-master/src/model.py:148: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "W0730 18:08:22.661986 140254510032704 deprecation_wrapper.py:119] From /home/xzhaoar/gpt-2-master/src/model.py:152: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "W0730 18:08:22.717363 140254510032704 deprecation_wrapper.py:119] From /home/xzhaoar/gpt-2-master/src/model.py:36: The name tf.rsqrt is deprecated. Please use tf.math.rsqrt instead.\n",
      "\n",
      "W0730 18:08:29.380876 140254510032704 deprecation.py:323] From /home/xzhaoar/anaconda3/envs/gpt2/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import model, sample, encoder\n",
    "\n",
    "\n",
    "\n",
    "model_name = '774M'\n",
    "models_dir = '../models'\n",
    "\n",
    "enc = encoder.get_encoder(model_name, models_dir)\n",
    "\n",
    "batch_size = 1\n",
    "seed=None\n",
    "nsamples=1\n",
    "\n",
    "\n",
    "hparams = model.default_hparams()\n",
    "with open(os.path.join(models_dir, model_name, 'hparams.json')) as f:\n",
    "    hparams.override_from_dict(json.load(f))\n",
    "    \n",
    "length = hparams.n_ctx // 2\n",
    "\n",
    "answer_collector = []\n",
    "\n",
    "\n",
    "def logits_score(logits,skeleton_tokens, context_tokens):\n",
    "    score = 1\n",
    "    start_index = len(skeleton_tokens) - 1 \n",
    "    end_index = len(context_tokens) - 1\n",
    "    \n",
    "    for i in range(end_index - start_index): \n",
    "        m = softmax(logits[start_index+i])\n",
    "        score *= m[context_tokens[start_index+i+1]]\n",
    "    \n",
    "    return score\n",
    "        \n",
    "\n",
    "\n",
    "with tf.Session(graph=tf.Graph()) as sess:\n",
    "    \n",
    "    context = tf.placeholder(tf.int32, [batch_size, None])\n",
    "    np.random.seed(seed)\n",
    "    tf.set_random_seed(seed)\n",
    "    \n",
    "    context_tokens = []\n",
    "\n",
    "    output = model.model(hparams=hparams, X=context, past=None, reuse=tf.AUTO_REUSE)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    ckpt = tf.train.latest_checkpoint(os.path.join(models_dir, model_name))\n",
    "    saver.restore(sess, ckpt)\n",
    "\n",
    "    for i in range(273):      \n",
    "        \n",
    "        if problems[i]['text']['txt1'] != \".\":\n",
    "            ans0 = problems[i]['answers'][0].replace(\"The\",\"the\")\n",
    "            ans1 = problems[i]['answers'][1].replace(\"The\",\"the\")\n",
    "        else:\n",
    "            ans0 = problems[i]['answers'][0]\n",
    "            ans1 = problems[i]['answers'][1]\n",
    "        \n",
    "        skeleton1 = problems[i]['text']['txt1'] + ' ' + problems[i]['answers'][0]\n",
    "        skeleton2 = problems[i]['text']['txt1'] + ' ' + problems[i]['answers'][1]\n",
    "        raw_text1 = problems[i]['text']['txt1'] + ' ' + problems[i]['answers'][0]  + ' ' + problems[i]['text']['txt2']\n",
    "        raw_text2 = problems[i]['text']['txt1'] + ' ' + problems[i]['answers'][1] + ' ' +  problems[i]['text']['txt2']\n",
    "        context_tokens1 = enc.encode(raw_text1)\n",
    "        context_tokens2 = enc.encode(raw_text2)\n",
    "        skeleton_tokens1 = enc.encode(skeleton1)\n",
    "        skeleton_tokens2 = enc.encode(skeleton2)\n",
    "        \n",
    "        out1 = sess.run(output, feed_dict={context: [context_tokens1 for _ in range(batch_size)]})\n",
    "        out2 = sess.run(output, feed_dict={context: [context_tokens2 for _ in range(batch_size)]})\n",
    "        \n",
    "        logits1 = out1['logits'][:, :, :hparams.n_vocab]\n",
    "        logits2 = out2['logits'][:, :, :hparams.n_vocab]\n",
    "            \n",
    "        score1 = logits_score(logits1[0],skeleton_tokens1,context_tokens1)\n",
    "        score2 = logits_score(logits2[0],skeleton_tokens2,context_tokens2)  \n",
    "        \n",
    "        correctAnswer = problems[i][\"correctAnswer\"]\n",
    "\n",
    "        if score1 >= score2:\n",
    "            predictedAnswer = \"A\"\n",
    "        else:\n",
    "            predictedAnswer = \"B\"\n",
    "        # A. Problem\n",
    "        answer_collector.append(predictedAnswer in correctAnswer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "273\n",
      "0.6923076923076923\n"
     ]
    }
   ],
   "source": [
    "print(len(answer_collector))\n",
    "print(np.sum(answer_collector)/273)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
