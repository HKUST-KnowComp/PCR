{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': {'txt1': 'The city councilmen refused the demonstrators a permit because', 'pron': 'they', 'txt2': 'feared violence.'}, 'answers': ['The city councilmen', 'The demonstrators'], 'correctAnswer': 'A', 'source': '(Winograd1972)'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guest/miniconda3/envs/gpt2/lib/python3.7/site-packages/ipykernel_launcher.py:18: DeprecationWarning: This method will be removed in future versions.  Use 'list(elem)' or iteration over elem instead.\n",
      "/home/guest/miniconda3/envs/gpt2/lib/python3.7/site-packages/ipykernel_launcher.py:23: DeprecationWarning: This method will be removed in future versions.  Use 'list(elem)' or iteration over elem instead.\n",
      "/home/guest/miniconda3/envs/gpt2/lib/python3.7/site-packages/ipykernel_launcher.py:31: DeprecationWarning: This method will be removed in future versions.  Use 'list(elem)' or iteration over elem instead.\n",
      "/home/guest/miniconda3/envs/gpt2/lib/python3.7/site-packages/ipykernel_launcher.py:25: DeprecationWarning: This method will be removed in future versions.  Use 'list(elem)' or iteration over elem instead.\n"
     ]
    }
   ],
   "source": [
    "# Load WSC dataset\n",
    "\n",
    "import xml.etree.ElementTree as etree\n",
    "# import spacy\n",
    "import json\n",
    "# from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "# from pandas import Series\n",
    "import logging\n",
    "import numpy\n",
    "# from nltk.corpus import wordnet as wn\n",
    "import os\n",
    "\n",
    "\n",
    "tree = etree.parse('WSCollection.xml')\n",
    "root = tree.getroot()\n",
    "original_problems = root.getchildren()\n",
    "problems = list()\n",
    "\n",
    "for original_problem in original_problems:\n",
    "    problem = dict()\n",
    "    for information in original_problem.getchildren():\n",
    "        if information.tag == 'answers':\n",
    "            answers = information.getchildren()\n",
    "            answer_list = list()\n",
    "            for answer in answers:\n",
    "                answer_list.append(answer.text.strip())\n",
    "            problem['answers'] = answer_list\n",
    "        elif information.tag == 'text':\n",
    "            texts = information.getchildren()\n",
    "            text_dict = dict()\n",
    "            for text1 in texts:\n",
    "                text_dict[text1.tag] = text1.text.replace('\\n', ' ').strip()\n",
    "            problem['text'] = text_dict\n",
    "        elif information.tag == 'quote':\n",
    "            pass\n",
    "        else:\n",
    "            problem[information.tag] = information.text.replace(' ', '')\n",
    "    problems.append(problem)\n",
    "\n",
    "print(problems[0])\n",
    "\n",
    "all_sentences = list()\n",
    "for question in problems:\n",
    "    sentence = question['text']['txt1'] + ' ' + question['text']['pron'] + ' ' + question['text']['txt2']\n",
    "    all_sentences.append(sentence)\n",
    "    # print(sentence)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guest/miniconda3/envs/gpt2/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/guest/miniconda3/envs/gpt2/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/guest/miniconda3/envs/gpt2/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/guest/miniconda3/envs/gpt2/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/guest/miniconda3/envs/gpt2/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/guest/miniconda3/envs/gpt2/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# Generate GPT answer example\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# import tensorflow.compat.v1 as tf\n",
    "# tf.disable_v2_behavior()\n",
    "import tensorflow as tf\n",
    "# from scipy.special import softmax\n",
    "import model, sample, encoder\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x)/sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# def interact_model(\n",
    "#     problems,\n",
    "#     answer_collection,\n",
    "#     full_s_c,\n",
    "#     keep_original,\n",
    "#     comparison,\n",
    "#     model_name='774M',\n",
    "#     seed=None,\n",
    "#     nsamples=1,\n",
    "#     batch_size=5,\n",
    "#     length=20,\n",
    "#     temperature=0.1,\n",
    "#     top_k=10,\n",
    "#     top_p=1,\n",
    "#     models_dir= '../models',\n",
    "# ):\n",
    "    \n",
    "#     # models_dir = os.path.expanduser(os.path.expandvars(models_dir))\n",
    "#     if batch_size is None:\n",
    "#         batch_size = 1\n",
    "#     # assert nsamples % batch_size == 0\n",
    "\n",
    "#     enc = encoder.get_encoder(model_name, models_dir)\n",
    "#     hparams = model.default_hparams()\n",
    "#     with open(os.path.join(models_dir, model_name, 'hparams.json')) as f:\n",
    "#         hparams.override_from_dict(json.load(f))\n",
    "\n",
    "#     if length is None:\n",
    "#         length = hparams.n_ctx // 2\n",
    "#     elif length > hparams.n_ctx:\n",
    "#         raise ValueError(\"Can't get samples longer than window size: %s\" % hparams.n_ctx)\n",
    "\n",
    "#     with tf.Session(graph=tf.Graph()) as sess:\n",
    "#         context = tf.placeholder(tf.int32, [batch_size, None])\n",
    "#         np.random.seed(seed)\n",
    "#         tf.set_random_seed(seed)\n",
    "#         output = sample.sample_sequence(\n",
    "#             hparams=hparams, length=length,\n",
    "#             context=context,\n",
    "#             batch_size=batch_size,\n",
    "#             temperature=temperature, top_k=top_k, top_p=top_p\n",
    "#         )\n",
    "\n",
    "#         saver = tf.train.Saver()\n",
    "#         ckpt = tf.train.latest_checkpoint(os.path.join(models_dir, model_name))\n",
    "#         saver.restore(sess, ckpt)\n",
    "\n",
    "#         for i in range(273):\n",
    "#             question = problems[i]\n",
    "            \n",
    "#             real_ans = \"\"\n",
    "            \n",
    "#             if \"A\" in  problems[i][\"correctAnswer\"] :\n",
    "#                 candT = problems[i][\"answers\"][0]\n",
    "#                 candF = problems[i][\"answers\"][1]\n",
    "#             else:\n",
    "#                 candT = problems[i][\"answers\"][1]\n",
    "#                 candF = problems[i][\"answers\"][0]\n",
    "                \n",
    "#             #  question['text']['pron']\n",
    "#             candT = candT.replace(\"The \", \"the \")\n",
    "#             candT = candT.replace(\"My \", \"my \")\n",
    "#             candF = candF.replace(\"The \", \"the \")\n",
    "#             candF = candF.replace(\"My \", \"my \")\n",
    "            \n",
    "#             if not keep_original:\n",
    "#                 raw_text = question['text']['txt1'] + ' ' + candT + ' ' + question['text']['txt2'] + \". Because\"\n",
    "#             # raw_text = \"The city councilmen refused the demonstrators a permit because the city councilmen feared violence. Because \"\n",
    "#             elif comparison == True:\n",
    "#                 sentence = question['text']['txt1'] + ' ' + question['text']['pron'] + ' ' + question['text']['txt2']\n",
    "#                 # relationship = \"The '\" + question['text']['pron'] +\"' is likely to refer to \"+candT +\" than \"+candF+\" because\"\n",
    "#                 relationship = \"The '\" + question['text']['pron'] +\"' is less likely to refer to \"+ candF+ \" than \"+candT+\" because\"\n",
    "#                 raw_text = sentence + relationship\n",
    "#             else:\n",
    "#                 sentence = question['text']['txt1'] + ' ' + question['text']['pron'] + ' ' + question['text']['txt2']\n",
    "#                 # relationship = \"The '\" + question['text']['pron'] +\"' is likely to refer to \"+candT +\" than \"+candF+\" because\"\n",
    "#                 relationship = \"The '\" + question['text']['pron'] +\"' is likely to refer to \"+candT +\" because\"\n",
    "#                 raw_text = sentence + relationship\n",
    "            \n",
    "#             context_tokens = enc.encode(raw_text)\n",
    "#             generated = 0\n",
    "#             for _ in range(nsamples // batch_size):\n",
    "#                 out = sess.run(output, feed_dict={\n",
    "#                     context: [context_tokens for _ in range(batch_size)]\n",
    "#                 })[:, len(context_tokens):]\n",
    "                \n",
    "#                 for j in range(batch_size):\n",
    "#                     generated += 1\n",
    "#                     text = enc.decode(out[j])\n",
    "                    \n",
    "#                     stripped_text = text[:(text.rfind(\".\") + 1)]\n",
    "                    \n",
    "#                     print(\"=\" * 40 + \" SAMPLE \" + str(i) + \" \" + \"=\" * 40)\n",
    "                    \n",
    "#                     if len(stripped_text) > 0:\n",
    "#                         text = stripped_text\n",
    "                    \n",
    "#                     # print(text)\n",
    "#                     print(raw_text + text)\n",
    "#                     full_s_c.append(raw_text + text)\n",
    "#                     answer_collection.append(text)\n",
    "            \n",
    "#             # print(\"=\" * 80)\n",
    "#         return answer_collection, full_s_c\n",
    "\n",
    "            \n",
    "            \n",
    "# answer_collection = []\n",
    "# full_s_c = []\n",
    "# ac, fsc = interact_model(problems, answer_collection, full_s_c, keep_original=True, comparison=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(fsc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('full_sent_collection_with_comparison.json', 'w') as f:\n",
    "#     json.dump(fsc,f)\n",
    "    \n",
    "# with open('append_sent_collection_with_comparison.json', 'w') as f:\n",
    "#     json.dump(ac,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import pickle  \n",
    "import spacy\n",
    "import json\n",
    "import numpy as np\n",
    "#import pandas as pd\n",
    "import logging\n",
    "import numpy\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import os\n",
    "\n",
    "import csv\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# import en_core_web_sm\n",
    "#!python -m spacy download en\n",
    "# !python -m spacy validate\n",
    "nlp = spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sents = []\n",
    "for i in range(273):      \n",
    "    # print(i)\n",
    "    all_sents.append(problems[i]['text']['txt1'] + ' ' + problems[i]['text']['txt2'])\n",
    "#     if problems[i]['text']['txt1'] != \".\":\n",
    "#         ans0 = problems[i]['answers'][0].replace(\"The\",\"the\")\n",
    "#         ans1 = problems[i]['answers'][1].replace(\"The\",\"the\")\n",
    "#     else:\n",
    "#         ans0 = problems[i]['answers'][0]\n",
    "#         ans1 = problems[i]['answers'][1]\n",
    "\n",
    "#     skeleton1 = problems[i]['text']['txt1'] + ' ' + problems[i]['answers'][0]\n",
    "#     skeleton2 = problems[i]['text']['txt1'] + ' ' + problems[i]['answers'][1]\n",
    "#     raw_text1 = problems[i]['text']['txt1'] + ' ' + problems[i]['answers'][0]  + ' ' + problems[i]['text']['txt2']\n",
    "#     raw_text2 = problems[i]['text']['txt1'] + ' ' + problems[i]['answers'][1] + ' ' +  problems[i]['text']['txt2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "sw = set(stopwords.words('english'))\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokenizer.tokenize('Eighty-seven miles to go, yet.  Onward!')\n",
    "\n",
    "# prune a type of POS\n",
    "def prune_pos(text, target):\n",
    "    doc = nlp(text)\n",
    "    temp = []\n",
    "    count = 0\n",
    "    for token in doc:\n",
    "        if token.text == \"_\":\n",
    "            temp.append(token.text)\n",
    "            \n",
    "        elif token.dep_  in target:\n",
    "            count = 1\n",
    "            continue\n",
    "        else:\n",
    "            # print(token.dep_)\n",
    "            temp.append(token.text)\n",
    "            \n",
    "    return \" \".join(temp[:-1]) + \".\", count\n",
    "\n",
    "def generate_pos_set(all_sents, target):\n",
    "    all_target = []\n",
    "    \n",
    "    for sent in all_sents:\n",
    "        doc = nlp(sent)\n",
    "        tars = [token.text for token in doc if token.pos_ == target]\n",
    "        try:\n",
    "            tars.remove(\"_\")\n",
    "        except:\n",
    "            a = \"nothing\"\n",
    "            \n",
    "        all_target.extend(tars)\n",
    "    return all_target\n",
    "\n",
    "\n",
    "def prune_token(text, token):\n",
    "    temp = text.replace(\" \"+ token + \" \",\" \")\n",
    "    temp = temp.replace(token + \" \",\" \")\n",
    "    temp = temp.replace(\" \" + token,\" \")\n",
    "    \n",
    "    return temp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Joan', 'Susan', 'Joan', 'Susan', 'Paul', 'George', 'Paul', 'George', 'Frank', 'Bill', 'Frank', 'Bill', 'styrofoam', 'John', 'Billy', 'John', 'Billy', 'Tom', 'Ray', 'Tom', 'Ray', 'Sue', 'Sally', 'Sue', 'Sally', 'Sam', 'Tina', 'Sam', 'Tina', 'Anna', 'Lucy', 'Anna', 'Lucy', 'Frank', 'Tom', 'Frank', 'Tom', 'Jim', 'Kevin', 'Jim', 'Kevin', 'Pete', 'Martin', 'Pete', 'Martin', 'Susan', 'Ann', 'Susan', 'Ann', 'Sid', 'Mark', 'Sid', 'Mark', 'Susan', 'Ann', 'Susan', 'Ann', 'Joe', 'Joe', 'Mark', 'Mark', 'Ann', 'Mary', 'Ann', 'Mary', 'Bob', 'Charlie', 'Bob', 'Charlie', 'Bob', 'Charlie', 'Charlie', 'Bob', 'Charlie', 'Charlie', 'Bob', 'Adam', 'Adam', 'Bob', 'Adam', 'Adam', 'Adam', 'Bob', 'Bob', 'Adam', 'Bob', 'Bob', 'Sam', 'Sam', 'Anne', 'Anne', 'Alice', 'Alice', 'Jim', 'Jim', 'keyhole', 'keyhole', 'John', 'John', 'John', 'John', 'Bob', 'Carl', 'Bob', 'Carl', 'Sam', 'Amy', 'Amy', 'Sam', 'Amy', 'Amy', 'Mark', 'Pete', 'Pete', 'Mark', 'Pete', 'Pete', 'Joe', 'Thursday', 'Joe', 'Thursday', 'Paul', 'Paul', 'Paul', 'Paul', 'Mary', 'Mary', 'Sam', 'Sam', 'Sara', 'Sara', 'Joey', 'Joey', 'Jane', 'Susan', 'Jane', 'Susan', 'Jane', 'Susan', 'Jane', 'Susan', 'Sam', 'Adam', 'Sam', 'Adam', 'ocean', 'beach', 'ocean', 'beach', 'Sam', 'Sam', 'Mary', 'Anne', 'Mary', 'Anne', 'Fred', 'Alice', 'Alaska', 'Fred', 'Alice', 'Alaska', 'Thomson', 'Cooper', 'Thomson', 'Cooper', 'Jackson', 'Arnold', 'Jackson', 'Arnold', 'Fred', 'Fred', 'Terpsichore', 'Tina', 'Terpsichore', 'Tina', 'Fred', 'George', 'Fred', 'George', 'Fred', 'Fred', 'Fred', 'Fred', 'Fred', 'Fred', 'Fred', 'Fred', 'July', 'Kamtchatka', 'Yakutsk', 'Yakutsk', 'July', 'Kamtchatka', 'Yakutsk', 'Yakutsk', 'Laputa', 'Laputa', 'Indian', 'Ocean', 'Indian', 'Ocean', 'Sam', 'Sam', 'Steve', 'Fred', 'Steve', 'Fred', 'John', 'Bill', 'John', 'Bill', 'John', 'Bill', 'John', 'Bill', 'Sam', 'Goodman', 'Spartan', 'Xenophanes', 'Sam', 'Goodman', 'Spartan', 'Xenophanes', 'Emma', 'Emma', 'Jane', 'Susan', 'Jane', 'Susan', 'Joe', 'Joe', 'Beth', 'Sally', 'Beth', 'Sally', 'Jim', 'Jim', 'Dan', 'Bill', 'Dibs', 'Dan', 'Bill', 'Dibs', 'Tom', 'Ralph', 'bishop', 'Tom', 'Ralph', 'bishop', 'Andrea', 'Susan', 'Andrea', 'Susan', 'Tom', 'Ralph', 'Tom', 'Ralph', 'Bill', 'John', 'Bill', 'John', 'Bill', 'John', 'Bill', 'John', 'Toby', 'Toby', 'Lily', 'Donna', 'Lily', 'Donna', 'Tommy', 'Timmy', 'father', 'Tommy', 'Timmy', 'father', 'Ollie', 'Tommy', 'Ollie', 'Tommy', 'Pam', 'Paul', 'Pam', 'Paul', 'Dr.', 'Adams', 'Kate', 'Dr.', 'Adams', 'Kate', 'Dan', 'Bill', 'Dan', 'Bill', 'George', 'Eric', 'George', 'Eric', 'George', 'Eric', 'Jane', 'Joan', 'Jane', 'Joan', 'James', 'Robert', 'James', 'Robert', 'Kirilov', 'Shatov', 'Kirilov', 'Shatov', 'Emma', 'Janie', 'Emma', 'Janie', 'Madonna', 'Madonna', 'Madonna', 'Madonna', 'Carol', 'Rebecca', 'Carol', 'Rebecca']\n"
     ]
    }
   ],
   "source": [
    "all_ADP = generate_pos_set(all_sents, \"PROPN\")\n",
    "\n",
    "print(all_ADP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pruned_tokens = [\" this \", \" that \"]\n",
    "# pruned_tokens = [\",\"]\n",
    "# pruned_tokens = [\".\"]\n",
    "# pruned_tokens = [\"the\", \"The\"]\n",
    "# pruned_tokens = [\"a\", \"A\"]\n",
    "# pruned_tokens = [\"this\", \"that\", \"these\", \"those\", \"This\", \"That\", \"These\", \"Those\"]\n",
    "# pruned_tokens = [\"has\", \"have\", \"had\", \"Has\", \"Have\", \"Had\"]\n",
    "# pruned_tokens = [\"Because\", \"because\"]\n",
    "# pruned_tokens = [\"go\", \"going\", \"went\", \"gone\", \"goes\"]\n",
    "# pruned_tokens = [\"but\", \"But\"]\n",
    "# pruned_tokens = [\"though\", \"although\", \"Though\", \"Although\",\"since\", \"Since\"] \n",
    "# pruned_tokens = [\"but\", \"But\", \"and\", \"And\"]\n",
    "# pruned_tokens = [\"very\", \"Very\"]\n",
    "# pruned_tokens = [\"so\", \"So\"]\n",
    "# pruned_tokens = [\"in\", \"In\"]\n",
    "# pruned_tokens = [\"of\", \"Of\"]\n",
    "# pruned_tokens = [\"with\", \"With\"]\n",
    "# pruned_tokens = [\"and\", \"or\", \"but\", \"as\", \"because\", \"for\", \"just as\", \"neither\", \"nor\", \"not only\", \"so\", \"whether\", \"yet\"]\n",
    "# pruned_tokens.extend([\"And\", \"Or\", \"But\", \"As\", \"Because\", \"For\", \"Just as\", \"Neither\", \"Nor\", \"Not only\", \"So\", \"Whether\", \"Yet\"])\n",
    "\n",
    "pruned_tokens = all_ADP\n",
    "\n",
    "# CONJS: And As Because But For Just as Or Neither Nor Not only So Whether Yet\n",
    "all_ADP.append('Kamchatka') #(for names)\n",
    "\n",
    "# new_ADP = []\n",
    "# for adp in all_ADP:\n",
    "#     new_ADP.append(adp.lower())\n",
    "# pruned_tokens = new_ADP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/guest/miniconda3/envs/gpt2/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/guest/miniconda3/envs/gpt2/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ../models/774M/model.ckpt\n",
      "158\n"
     ]
    }
   ],
   "source": [
    "# print(all_sentences[0])\n",
    "# print(all_sentences[1])\n",
    "# print(all_sentences[2])\n",
    "\n",
    "# print(problems)\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# from scipy.special import softmax\n",
    "import model, sample, encoder\n",
    "\n",
    "\n",
    "\n",
    "model_name = '774M'\n",
    "models_dir = '../models'\n",
    "\n",
    "enc = encoder.get_encoder(model_name, models_dir)\n",
    "\n",
    "batch_size = 1\n",
    "seed=None\n",
    "nsamples=1\n",
    "\n",
    "\n",
    "hparams = model.default_hparams()\n",
    "with open(os.path.join(models_dir, model_name, 'hparams.json')) as f:\n",
    "    hparams.override_from_dict(json.load(f))\n",
    "    \n",
    "length = hparams.n_ctx // 2\n",
    "\n",
    "answer_collector = []\n",
    "related_answer_collector = []\n",
    "\n",
    "\n",
    "def logits_score(logits,skeleton_tokens, context_tokens):\n",
    "    score = 1\n",
    "    start_index = len(skeleton_tokens) - 1 \n",
    "    end_index = len(context_tokens) - 1\n",
    "    \n",
    "    for i in range(end_index - start_index): \n",
    "        m = softmax(logits[start_index+i])\n",
    "        score *= m[context_tokens[start_index+i+1]]\n",
    "    \n",
    "    return score\n",
    "        \n",
    "num_case = 0\n",
    "\n",
    "with tf.Session(graph=tf.Graph()) as sess:\n",
    "    \n",
    "    context = tf.placeholder(tf.int32, [batch_size, None])\n",
    "    np.random.seed(seed)\n",
    "    tf.set_random_seed(seed)\n",
    "    \n",
    "    context_tokens = []\n",
    "\n",
    "        # print(context_tokens)\n",
    "\n",
    "    output = model.model(hparams=hparams, X=context, past=None, reuse=tf.AUTO_REUSE)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    ckpt = tf.train.latest_checkpoint(os.path.join(models_dir, model_name))\n",
    "    saver.restore(sess, ckpt)\n",
    "\n",
    "        # out = sess.run(output, feed_dict={context: context_tokens})\n",
    "        \n",
    "    for i in range(273):      \n",
    "        # print(i)\n",
    "        \n",
    "        if problems[i]['text']['txt1'] != \".\":\n",
    "            ans0 = problems[i]['answers'][0].replace(\"The\",\"the\")\n",
    "            ans1 = problems[i]['answers'][1].replace(\"The\",\"the\")\n",
    "        else:\n",
    "            ans0 = problems[i]['answers'][0]\n",
    "            ans1 = problems[i]['answers'][1]\n",
    "            \n",
    "        cand0 = problems[i]['answers'][0]\n",
    "        cand1 = problems[i]['answers'][1]\n",
    "        \n",
    "        skeleton1 = problems[i]['text']['txt1'] + ' ' + \"_\"\n",
    "        skeleton2 = problems[i]['text']['txt1'] + ' ' + \"_\"\n",
    "        raw_text1 = problems[i]['text']['txt1'] + ' ' + \"_\"  + ' ' + problems[i]['text']['txt2']\n",
    "        raw_text2 = problems[i]['text']['txt1'] + ' ' + \"_\"  + ' ' +  problems[i]['text']['txt2']\n",
    "        \n",
    "        # Start the pruning process\n",
    "        \n",
    "        # pruning the cands (besides names)\n",
    "        pruned_tokens=[]\n",
    "\n",
    "        pruned_tokens.extend(tokenizer.tokenize(cand0))\n",
    "        pruned_tokens.extend(tokenizer.tokenize(cand1))\n",
    "\n",
    "        names = []\n",
    "        for name in all_ADP:\n",
    "            try:\n",
    "                pruned_tokens.remove(name)\n",
    "                names.append(name)\n",
    "            except:\n",
    "                a = \"nothing\"\n",
    "                \n",
    "        temp_case = 0\n",
    "        for token in pruned_tokens:\n",
    "#             skeleton1 = prune_token(skeleton1, token)\n",
    "#             skeleton2 = prune_token(skeleton2, token)\n",
    "#             raw_text1 = prune_token(raw_text1, token)\n",
    "#             raw_text2 = prune_token(raw_text2, token)\n",
    "\n",
    "            if raw_text1.find(\" \"+ token + \" \") != -1 or raw_text1.find(token + \" \") != -1:\n",
    "                temp_case = 1\n",
    "\n",
    "\n",
    "        # pruning with dependency structure...\n",
    "        # \"dobj\", \"pobj\"\n",
    "        # \"nsubj\"\n",
    "        # \"root\"\n",
    "        # \"det\"\n",
    "        # \"neg\"\n",
    "        # \"amod\", \"advmod\", \"npadvmod\"\n",
    "        \n",
    "#         skeleton1,temp_case = prune_pos(skeleton1, [\"neg\"])\n",
    "#         skeleton2,temp_case = prune_pos(skeleton2, [\"neg\"])\n",
    "#         raw_text1,temp_case = prune_pos(raw_text1, [\"neg\"])\n",
    "#         raw_text2,temp_case = prune_pos(raw_text2, [\"neg\"])\n",
    "                \n",
    "        \n",
    "        # special case for names\n",
    "        skeleton1 = skeleton1.replace(\"_\", cand0)\n",
    "        skeleton2 = skeleton2.replace(\"_\", cand1)\n",
    "        raw_text1 = raw_text1.replace(\"_\", cand0)\n",
    "        raw_text2 = raw_text2.replace(\"_\", cand1)\n",
    "        \n",
    "\n",
    "        \n",
    "        num_case +=  temp_case\n",
    "        \n",
    "        context_tokens1 = enc.encode(raw_text1)\n",
    "        context_tokens2 = enc.encode(raw_text2)\n",
    "        skeleton_tokens1 = enc.encode(skeleton1)\n",
    "        skeleton_tokens2 = enc.encode(skeleton2)\n",
    "        \n",
    "        out1 = sess.run(output, feed_dict={context: [context_tokens1 for _ in range(batch_size)]})\n",
    "        out2 = sess.run(output, feed_dict={context: [context_tokens2 for _ in range(batch_size)]})\n",
    "        \n",
    "        logits1 = out1['logits'][:, :, :hparams.n_vocab]\n",
    "        logits2 = out2['logits'][:, :, :hparams.n_vocab]\n",
    "        # presents = out['present']    \n",
    "    \n",
    "            \n",
    "        score1 = logits_score(logits1[0],skeleton_tokens1,context_tokens1)\n",
    "        score2 = logits_score(logits2[0],skeleton_tokens2,context_tokens2)\n",
    "        \n",
    "        \n",
    "        correctAnswer = problems[i][\"correctAnswer\"]\n",
    "\n",
    "        if score1 >= score2:\n",
    "            predictedAnswer = \"A\"\n",
    "        else:\n",
    "            predictedAnswer = \"B\"\n",
    "        # A. Problem\n",
    "        answer_collector.append(predictedAnswer in correctAnswer)\n",
    "        \n",
    "print(num_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189\n",
      "0.6923076923076923\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(answer_collector))\n",
    "print(np.sum(answer_collector)/273)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(answer_collector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"gpt2_answers.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(answer_collector,f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
