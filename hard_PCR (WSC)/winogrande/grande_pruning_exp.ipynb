{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import pickle  \n",
    "import spacy\n",
    "import json\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "import logging\n",
    "import numpy\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import os\n",
    "\n",
    "import csv\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Eighty', 'seven', 'miles', 'to', 'go', 'yet', 'Onward']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "sw = set(stopwords.words('english'))\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokenizer.tokenize('Eighty-seven miles to go, yet.  Onward!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filename):\n",
    "    # read a winogrande style jsonl file\n",
    "    # qID, sentence, option1, option2, answer\n",
    "    this_set = []\n",
    "    \n",
    "    \n",
    "    with open(filename,\"r\") as json_file:\n",
    "        json_list = list(json_file)\n",
    "\n",
    "        for json_str in json_list:\n",
    "            result = json.loads(json_str)\n",
    "            this_set.append(result)\n",
    "    print(\"Loaded \"+ filename + \" with \"+ str(len(this_set)) + \" items.\")\n",
    "    return this_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded wsc.jsonl with 279 items.\n"
     ]
    }
   ],
   "source": [
    "wsc = load_dataset(\"wsc.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sents = []\n",
    "for i in range(len(wsc)):\n",
    "    #print(wsc[i])\n",
    "    all_sents.append(wsc[i][\"sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('_', 279), ('bill', 16), ('sam', 16), ('fred', 16), ('susan', 14), ('man', 14), ('john', 14), ('time', 14), ('put', 14), ('bob', 12), ('see', 11), ('years', 11), ('got', 11), ('gave', 11), ('tried', 10), ('tree', 10), ('work', 9), ('took', 9), ('saw', 9), ('get', 9), ('book', 9), ('paul', 8), ('tom', 8), ('much', 8), ('better', 8), ('one', 8), ('away', 8), ('would', 8), ('adam', 8), ('alice', 8), ('since', 8), ('knocked', 8), ('jane', 8), ('ago', 8), ('father', 8), ('george', 7), ('good', 7), ('though', 7), ('woman', 7), ('asked', 6), ('right', 6), ('table', 6), ('look', 6), ('lot', 6), ('jim', 6), ('pete', 6), ('full', 6), ('ann', 6), ('mark', 6), ('joe', 6), ('still', 6), ('started', 6), ('came', 6), ('mary', 6), ('library', 6), ('paid', 6), ('charlie', 6), ('home', 6), ('daughter', 6), ('carried', 6), ('door', 6), ('turned', 6), ('long', 6), ('passed', 6), ('boyfriend', 6), ('answer', 5), ('although', 5), ('police', 5), ('moved', 5), ('house', 5), ('education', 5), ('run', 5), ('left', 5), ('stop', 5), ('many', 5), ('wind', 5), ('place', 5), ('refused', 4), ('fit', 4), ('joan', 4), ('made', 4), ('sure', 4), ('help', 4), ('school', 4), ('frank', 4), ('lift', 4), ('son', 4), ('ball', 4), ('crashed', 4), ('stage', 4), ('billy', 4), ('front', 4), ('ran', 4), ('beat', 4), ('sally', 4), ('start', 4), ('shelf', 4), ('tina', 4), ('friend', 4), ('coming', 4)]\n"
     ]
    }
   ],
   "source": [
    "# find the most frequent tokens\n",
    "from collections import Counter\n",
    "\n",
    "words = []\n",
    "\n",
    "for sent in all_sents:\n",
    "    temp_words = tokenizer.tokenize(sent.lower())\n",
    "    for temp_word in temp_words:\n",
    "        \n",
    "        if temp_word not in sw:\n",
    "            words.append(temp_word)\n",
    "        #words.append(temp_word)\n",
    "\n",
    "counter = Counter()\n",
    "counter.update(words)\n",
    "most_common = counter.most_common(100)\n",
    "print(most_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prune a type of dependency token\n",
    "def prune_pos(text, target):\n",
    "    doc = nlp(text)\n",
    "    temp = []\n",
    "    count = 0\n",
    "    for token in doc:\n",
    "        if token.text == \"_\":\n",
    "            temp.append(token.text)\n",
    "            \n",
    "        elif token.dep_  in target:\n",
    "            count = 1\n",
    "            continue\n",
    "        else:\n",
    "            # print(token.dep_)\n",
    "            temp.append(token.text)\n",
    "            \n",
    "    return \" \".join(temp[:-1]) + \".\", count\n",
    "\n",
    "def generate_pos_set(all_sents, target):\n",
    "    all_target = []\n",
    "    \n",
    "    for sent in all_sents:\n",
    "        doc = nlp(sent)\n",
    "        tars = [token.text for token in doc if token.pos_ == target]\n",
    "        \n",
    "        try:\n",
    "            tars.remove(\"_\")\n",
    "        except:\n",
    "            a = \"nothing\"\n",
    "            \n",
    "        all_target.extend(tars)\n",
    "        \n",
    "    return all_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_POS = generate_pos_set(all_sents, \"VERB\")\n",
    "# all_POS = generate_pos_set(all_sents, \"PRON\")\n",
    "# all_POS = generate_pos_set(all_sents, \"ADJ\")\n",
    "# all_POS = generate_pos_set(all_sents, \"ADV\")\n",
    "\n",
    "all_POS = generate_pos_set(all_sents, \"PROPN\") # names\n",
    "all_POS.append('Kamchatka')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Joan', 'Susan', 'Joan', 'Susan', 'Paul', 'George', 'Paul', 'George', 'Frank', 'Bill', 'Frank', 'Bill', 'styrofoam', 'John', 'Billy', 'John', 'Billy', 'Tom', 'Ray', 'Tom', 'Ray', 'Sue', 'Sally', 'Sue', 'Sally', 'Sam', 'Tina', 'Sam', 'Tina', 'Anna', 'Lucy', 'Anna', 'Lucy', 'Frank', 'Tom', 'Frank', 'Tom', 'Jim', 'Kevin', 'Jim', 'Kevin', 'Pete', 'Martin', 'Pete', 'Martin', 'Susan', 'Ann', 'Susan', 'Ann', 'Sid', 'Mark', 'Sid', 'Mark', 'Susan', 'Ann', 'Susan', 'Ann', 'Joe', 'Joe', 'Mark', 'Mark', 'Ann', 'Mary', 'Ann', 'Mary', 'Bob', 'Charlie', 'Bob', 'Charlie', 'Bob', 'Charlie', 'Charlie', 'Bob', 'Charlie', 'Charlie', 'Bob', 'Adam', 'Adam', 'Bob', 'Adam', 'Adam', 'Adam', 'Bob', 'Bob', 'Adam', 'Bob', 'Bob', 'Sam', 'Sam', 'Anne', 'Anne', 'Alice', 'Alice', 'Jim', 'Jim', 'keyhole', 'keyhole', 'John', 'John', 'John', 'John', 'Bob', 'Carl', 'Bob', 'Carl', 'Sam', 'Amy', 'Amy', 'Sam', 'Amy', 'Amy', 'Mark', 'Pete', 'Pete', 'Mark', 'Pete', 'Pete', 'Joe', 'Thursday', 'Joe', 'Thursday', 'Paul', 'Paul', 'Paul', 'Paul', 'Mary', 'Mary', 'Sam', 'Sam', 'Sara', 'Sara', 'Joey', 'Joey', 'Jane', 'Susan', 'Jane', 'Susan', 'Jane', 'Susan', 'Jane', 'Susan', 'Sam', 'Adam', 'Sam', 'Adam', 'ocean', 'ocean', 'Sam', 'Sam', 'Mary', 'Anne', 'Mary', 'Anne', 'Fred', 'Alice', 'Alaska', 'Fred', 'Alice', 'Alaska', 'Thomson', 'Cooper', 'Thomson', 'Cooper', 'Jackson', 'Arnold', 'Jackson', 'Arnold', 'Fred', 'Fred', 'Terpsichore', 'Tina', 'Terpsichore', 'Tina', 'Fred', 'George', 'Fred', 'George', 'Fred', 'Fred', 'Fred', 'Fred', 'Fred', 'Fred', 'Fred', 'Fred', 'July', 'Kamtchatka', 'Yakutsk', 'Yakutsk', 'July', 'Kamtchatka', 'Yakutsk', 'Yakutsk', 'Laputa', 'Laputa', 'Indian', 'Ocean', 'Indian', 'Ocean', 'Sam', 'Sam', 'Steve', 'Fred', 'Steve', 'Fred', 'John', 'Bill', 'John', 'Bill', 'John', 'Bill', 'John', 'Bill', 'Sam', 'Goodman', 'Spartan', 'Xenophanes', 'Sam', 'Goodman', 'Spartan', 'Xenophanes', 'Emma', 'Emma', 'Jane', 'Susan', 'Jane', 'Susan', 'Joe', 'Joe', 'Beth', 'Sally', 'Beth', 'Sally', 'Jim', 'Jim', 'Dan', 'Bill', 'Dibs', 'Dan', 'Bill', 'Dibs', 'Tom', 'Ralph', 'Tom', 'Ralph', 'Andrea', 'Susan', 'Andrea', 'Susan', 'Tom', 'Ralph', 'Tom', 'Ralph', 'Bill', 'John', 'Bill', 'John', 'Bill', 'John', 'Bill', 'John', 'Patting', 'Toby', 'Toby', 'Lily', 'Donna', 'Lily', 'Donna', 'Tommy', 'Timmy', 'father', 'Tommy', 'Timmy', 'father', 'Ollie', 'Tommy', 'Ollie', 'Tommy', 'Pam', 'Paul', 'Pam', 'Paul', 'Dr.', 'Adams', 'Kate', 'Dr.', 'Adams', 'Kate', 'Dan', 'Bill', 'Dan', 'Bill', 'George', 'Eric', 'George', 'Eric', 'George', 'Eric', 'Jane', 'Joan', 'Jane', 'Joan', 'James', 'Robert', 'James', 'Robert', 'Kirilov', 'Shatov', 'Kirilov', 'Shatov', 'Emma', 'Janie', 'Emma', 'Janie', 'Madonna', 'Madonna', 'Madonna', 'Madonna', 'Carol', 'Rebecca', 'Carol', 'Rebecca', 'Shakespeare', 'Ovid', 'Shakespeare', 'Goethe', 'Shakespeare', 'Ovid', 'Shakespeare', 'Goethe', 'Alice', 'Jade', 'Alice', 'Alice', 'Jade', 'Alice', 'Kamchatka']\n"
     ]
    }
   ],
   "source": [
    "print(all_POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of involved cases: 184\n"
     ]
    }
   ],
   "source": [
    "# Pruning\n",
    "pruned_wsc = []\n",
    "\n",
    "# pruned_tokens = [\" this \", \" that \"]\n",
    "# pruned_tokens = [\",\"]\n",
    "# pruned_tokens = [\".\"]\n",
    "# pruned_tokens = [\"the\", \"The\"]\n",
    "# pruned_tokens = [\"a\", \"A\"]\n",
    "# pruned_tokens = [\"this\", \"that\", \"these\", \"those\", \"This\", \"That\", \"These\", \"Those\"]\n",
    "# pruned_tokens = [\"has\", \"have\", \"had\", \"Has\", \"Have\", \"Had\"]\n",
    "# pruned_tokens = [\"Because\", \"because\"]\n",
    "# pruned_tokens = [\"go\", \"going\", \"went\", \"gone\", \"goes\"]\n",
    "# pruned_tokens = [\"but\", \"But\"]\n",
    "# pruned_tokens = [\"though\", \"although\", \"Though\", \"Although\",\"since\", \"Since\"] \n",
    "# pruned_tokens = [\"but\", \"But\", \"and\", \"And\"]\n",
    "# pruned_tokens = [\"very\", \"Very\"]\n",
    "# pruned_tokens = [\"so\", \"So\"]\n",
    "# pruned_tokens = [\"in\", \"In\"]\n",
    "# pruned_tokens = [\"of\", \"Of\"]\n",
    "# pruned_tokens = [\"on\", \"On\"]\n",
    "# pruned_tokens = [\"with\", \"With\"]\n",
    "# pruned_tokens = [\"and\", \"or\", \"but\", \"as\", \"because\", \"for\", \"just as\", \"neither\", \"nor\", \"not only\", \"so\", \"whether\", \"yet\"]\n",
    "# pruned_tokens.extend([\"And\", \"Or\", \"But\", \"As\", \"Because\", \"For\", \"Just as\", \"Neither\", \"Nor\", \"Not only\", \"So\", \"Whether\", \"Yet\"])\n",
    "\n",
    "\n",
    "\n",
    "new_POS = []\n",
    "for pos in all_POS:\n",
    "    new_POS.append(pos.lower())\n",
    "    \n",
    "pruned_tokens = new_POS\n",
    "\n",
    "num_case = []\n",
    "\n",
    "for piece in wsc:\n",
    "    new_piece = {}\n",
    "    for key, value in piece.items():\n",
    "        if key != \"sentence\":\n",
    "            new_piece[key] = value\n",
    "        else:\n",
    "            temp = value.lower()\n",
    "            if value[-1] != \".\":\n",
    "                value += \".\"\n",
    "                \n",
    "            temp_case = 0\n",
    "            \n",
    "            ###### uncomment this block if you want to prune candidates...###\n",
    "            #################################################################\n",
    "            # pruned_tokens=[]\n",
    "            # cand1 = piece[\"option1\"]\n",
    "            # cand2 = piece[\"option2\"]\n",
    "            # pruned_tokens.extend(tokenizer.tokenize(cand1.lower()))\n",
    "            # pruned_tokens.extend(tokenizer.tokenize(cand2.lower()))\n",
    "\n",
    "            # names = []\n",
    "            # for name in new_ADP:\n",
    "            #     try:\n",
    "            #         pruned_tokens.remove(name)\n",
    "            #         names.append(name)\n",
    "            #     except:\n",
    "            #         a = \"nothing\"\n",
    "\n",
    "            # # pruned_tokens = names\n",
    "            # print(pruned_tokens)\n",
    "            \n",
    "            # if len(pruned_tokens) > 0:\n",
    "                # temp_case = 1\n",
    "            ################################################################\n",
    "            \n",
    "            for token in pruned_tokens:\n",
    "                if temp.find(\" \"+ token + \" \") != -1 or temp.find(token + \" \") != -1 or temp.find( \" \"+token) != -1:\n",
    "                    temp_case = 1\n",
    "\n",
    "                temp = temp.replace(\" \"+ token + \" \",\" \")\n",
    "                temp = temp.replace(token + \" \",\" \")\n",
    "                temp = temp.replace(\" \" + token,\" \")\n",
    "            \n",
    "            # print(temp)\n",
    "            new_piece[key] = temp\n",
    "                \n",
    "            num_case.append(temp_case)\n",
    "            \n",
    "            ###### uncomment this block if you want to prune by dep token ###\n",
    "            #################################################################      \n",
    "            \n",
    "            # new_piece[key], temp_count=prune_pos(value.lower(), [\"amod\", \"advmod\", \"npadvmod\"])\n",
    "            # new_piece[key], temp_count=prune_pos(value.lower(), [\"neg\"])\n",
    "            # num_case.append(temp_count)\n",
    "            \n",
    "    pruned_wsc.append(new_piece)\n",
    "                \n",
    "print(\"Number of involved cases:\",sum(num_case))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sudo_set(cover_set, output_folder):\n",
    "    # generate the seleted/unselected set\n",
    "    with open(output_folder+\"/dev.jsonl\",\"w\") as f1:\n",
    "        with open(output_folder+\"/wsc.jsonl\", \"w\") as f2:\n",
    "            for i,piece in enumerate(cover_set):\n",
    "                # if i in selected_ids:\n",
    "                f1.write(json.dumps(piece))\n",
    "                f1.write(\"\\n\")\n",
    "                f2.write(json.dumps(piece))\n",
    "                f2.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the generated set could be used to evaluate winogrande\n",
    "# generate_sudo_set(pruned_wsc, \"./prune\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
